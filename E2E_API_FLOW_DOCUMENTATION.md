# ProfAI_PROD End-to-End API Flow Documentation

## Overview

This document provides a comprehensive mapping of the end-to-end API flows in the ProfAI_PROD system, detailing every file, function, and service involved from initial API trigger to final response delivery. The system is a multilingual AI-powered educational assistant with PDF processing, course generation, RAG-based chat, and voice interaction capabilities.

## System Architecture Summary

### Core Components
- **Frontend**: Web-based interface (`web/` directory)
- **API Layer**: FastAPI application (`app.py`)
- **Services Layer**: Business logic services (`services/` directory)
- **Core Layer**: Course generation and vectorization (`core/` directory)
- **Processors Layer**: Document and text processing (`processors/` directory)
- **Models Layer**: Pydantic schemas (`models/` directory)
- **Utils Layer**: Connection monitoring and utilities (`utils/` directory)

### External Integrations
- **OpenAI GPT-4o-mini**: Content generation and general LLM tasks
- **Sarvam AI**: Multilingual TTS/STT/translation (11 Indian languages)
- **ChromaDB/FAISS**: Vector storage for RAG
- **Groq API**: RAG-based question answering

---

## E2E Flow #0: System Startup & Initialization

### **Entry Point**: `run_profai_websocket.py`

This is the **primary entry point** for the ProfAI_PROD system that starts both FastAPI and WebSocket servers.

### **Startup Sequence**

#### 1. Main Entry Point
**File**: `run_profai_websocket.py`  
**Function**: `main()`  
**Lines**: 31-96

```python
def main()
â”œâ”€â”€ Port availability check for 5001 (FastAPI) and 8765 (WebSocket)
â”œâ”€â”€ Starts FastAPI server in background thread via start_fastapi_server()
â”œâ”€â”€ Starts WebSocket server via start_websocket_server()
â””â”€â”€ Handles graceful shutdown on KeyboardInterrupt
```

#### 2. FastAPI Server Initialization
**Function**: `start_fastapi_server()`  
**Lines**: 16-22

```python
def start_fastapi_server()
â”œâ”€â”€ Imports uvicorn and app from app.py
â”œâ”€â”€ Starts FastAPI server on host="0.0.0.0", port=5001
â””â”€â”€ Makes web interface available at http://localhost:5001
```

**File**: `app.py`  
**Lines**: 44-82
```python
# FastAPI App Initialization
app = FastAPI(title="ProfAI API", version="2.0.0")
â”œâ”€â”€ Configures CORS middleware (allow_origins=["*"])
â”œâ”€â”€ Mounts static files from web/ directory
â”œâ”€â”€ Initializes all services:
â”‚   â”œâ”€â”€ ChatService() - RAG-based conversations
â”‚   â”œâ”€â”€ DocumentService() - PDF processing
â”‚   â”œâ”€â”€ AudioService() - TTS/STT operations
â”‚   â””â”€â”€ TeachingService() - Educational content delivery
â””â”€â”€ Registers API endpoints for PDF upload and processing
```

#### 3. WebSocket Server Initialization
**Function**: `start_websocket_server()`  
**Lines**: 24-29

```python
def start_websocket_server()
â”œâ”€â”€ Imports websocket_server.start_websocket_server
â”œâ”€â”€ Starts WebSocket server on host="0.0.0.0", port=8765
â””â”€â”€ Makes WebSocket interface available at ws://localhost:8765
```

**File**: `websocket_server.py`  
**Function**: `start_websocket_server()`
```python
async def start_websocket_server(host, port)
â”œâ”€â”€ Initializes WebSocket connection handler
â”œâ”€â”€ Sets up client connection management
â”œâ”€â”€ Registers message handlers for different message types
â”œâ”€â”€ Starts listening for WebSocket connections
â””â”€â”€ Handles client connections via handle_client() function
```

---

## E2E Flow #1: PDF Upload & Course Generation

### **Trigger**: `POST /api/upload-pdfs`

### **Entry Point**
**File**: `app.py`  
**Function**: `upload_and_process_pdfs()`  
**Lines**: 86-108

### **Flow Sequence**

#### 1. Request Validation & File Handling
```python
# app.py:86-108
async def upload_and_process_pdfs(files: List[UploadFile], course_title: str)
â”œâ”€â”€ Validates service availability (SERVICES_AVAILABLE check)
â”œâ”€â”€ Calls document_service.process_pdfs_and_generate_course()
```

#### 2. Document Service Processing
**File**: `services/document_service.py`  
**Function**: `DocumentService.process_pdfs_and_generate_course()`  
**Lines**: 24-26
```python
async def process_pdfs_and_generate_course() 
â””â”€â”€ Delegates to process_uploaded_pdfs()
```

**Function**: `DocumentService.process_uploaded_pdfs()`  
**Lines**: 28-96
```python
def process_uploaded_pdfs(pdf_files, course_title)
â”œâ”€â”€ Clears/prepares documents directory (config.DOCUMENTS_DIR)
â”œâ”€â”€ Saves uploaded PDFs to disk
â”œâ”€â”€ STEP 1: Extract text from PDFs
â”œâ”€â”€ STEP 2: Chunk documents  
â”œâ”€â”€ STEP 3: Create vector store
â”œâ”€â”€ STEP 4: Generate course
â””â”€â”€ STEP 5: Save course output
```

#### 3. PDF Text Extraction
**File**: `processors/pdf_extractor.py`  
**Class**: `PDFExtractor`  
**Function**: `extract_text_from_directory()`  
**Lines**: 37-69
```python
def extract_text_from_directory(directory_path)
â”œâ”€â”€ Iterates through PDF files in directory
â”œâ”€â”€ Calls _extract_from_pdf() for each PDF
â””â”€â”€ Returns List[Dict[str, str]] with source and content
```

**Function**: `_extract_from_pdf()`  
**Lines**: 14-24
```python
def _extract_from_pdf(file_path)
â”œâ”€â”€ Uses PyPDF2.PdfReader to read PDF
â”œâ”€â”€ Extracts text from each page
â””â”€â”€ Returns concatenated text content
```

#### 4. Text Chunking
**File**: `processors/text_chunker.py`  
**Class**: `TextChunker`  
**Function**: `chunk_documents()`  
**Lines**: 20-41
```python
def chunk_documents(documents: List[Dict[str, str]])
â”œâ”€â”€ Uses RecursiveCharacterTextSplitter
â”œâ”€â”€ Splits each document into chunks
â”œâ”€â”€ Creates Document objects with metadata
â””â”€â”€ Returns List[Document] with chunked content
```

#### 5. Vector Store Creation
**File**: `core/vectorizer.py`  
**Class**: `Vectorizer`  
**Function**: `create_vector_store()`  
**Lines**: 18-31
```python
def create_vector_store(chunks: List[Document])
â”œâ”€â”€ Uses OpenAIEmbeddings for embeddings
â”œâ”€â”€ Creates FAISS vector store from documents
â””â”€â”€ Returns FAISS vectorstore object
```

**Function**: `save_vector_store()`  
**Lines**: 33-43
```python
def save_vector_store(vector_store, path)
â”œâ”€â”€ Creates directory structure
â”œâ”€â”€ Saves FAISS vector store to local path
â””â”€â”€ Persists for future retrieval operations
```

#### 6. Course Generation
**File**: `core/course_generator.py`  
**Class**: `CourseGenerator`  
**Function**: `generate_course()`  
**Lines**: 32-50
```python
def generate_course(documents, retriever, course_title)
â”œâ”€â”€ Step 1: Generate curriculum structure (_generate_curriculum)
â””â”€â”€ Step 2: Generate content for each topic (_generate_content)
```

**Function**: `_generate_curriculum()`  
**Lines**: 52-95
```python
def _generate_curriculum(documents, course_title)
â”œâ”€â”€ Creates context from all documents
â”œâ”€â”€ Uses ChatOpenAI with curriculum generation model
â”œâ”€â”€ Applies curriculum prompt template
â”œâ”€â”€ Uses JsonOutputParser with CourseLMS schema
â””â”€â”€ Returns structured CourseLMS curriculum
```

**Function**: `_generate_content()`  
**Lines**: 97-154
```python
def _generate_content(curriculum, retriever)
â”œâ”€â”€ For each module and sub-topic in curriculum:
â”‚   â”œâ”€â”€ Retrieves relevant documents using retriever
â”‚   â”œâ”€â”€ Creates context-aware content generation prompt
â”‚   â”œâ”€â”€ Uses ChatOpenAI content generation model
â”‚   â””â”€â”€ Generates detailed content for sub-topic
â””â”€â”€ Returns complete CourseLMS with content
```

#### 7. Response Formation & Persistence
**File**: `services/document_service.py`  
**Lines**: 85-92
```python
# Save course output
â”œâ”€â”€ Creates courses directory (config.COURSES_DIR)
â”œâ”€â”€ Saves course JSON to OUTPUT_JSON_PATH
â””â”€â”€ Returns final_course.dict() as API response
```

### **Models Used**
**File**: `models/schemas.py`  
- `CourseLMS`: Main course structure (lines 19-22)
- `Module`: Week-based module structure (lines 13-17)  
- `SubTopic`: Individual topic with content (lines 8-11)

### **Configuration Dependencies**
**File**: `config.py`
- `DOCUMENTS_DIR`: PDF storage location
- `VECTORSTORE_DIR`: Vector embeddings storage  
- `COURSES_DIR`: Generated course output location
- `CHUNK_SIZE`, `CHUNK_OVERLAP`: Text chunking parameters
- `EMBEDDING_MODEL_NAME`: OpenAI embedding model
- `CURRICULUM_GENERATION_MODEL`: GPT model for curriculum
- `CONTENT_GENERATION_MODEL`: GPT model for content

---

## E2E Flow #2: WebSocket Chat with RAG

### **Trigger**: WebSocket message sent to `ws://localhost:8765`

### **Entry Point**
**File**: `websocket_server.py`  
**Function**: `handle_client(websocket, path)`

**WebSocket Connection**: Client connects to `ws://localhost:8765`

### **Complete E2E Flow Sequence**

#### 0. WebSocket Connection & Message Reception
**File**: `websocket_server.py`
```python
async def handle_client(websocket, path)
â”œâ”€â”€ Accepts WebSocket connection
â”œâ”€â”€ Generates unique client_id using timestamp + random
â”œâ”€â”€ Logs connection with client_id
â”œâ”€â”€ Enters message receiving loop:
â”‚   â”œâ”€â”€ Receives JSON message from client
â”‚   â”œâ”€â”€ Parses message type and content
â”‚   â””â”€â”€ Routes to appropriate handler based on message type
â”œâ”€â”€ Handles connection errors and disconnections
â””â”€â”€ Cleans up resources on disconnect
```

#### 1. Chat Request Processing
**File**: `services/chat_service.py`  
**Class**: `ChatService`  
**Function**: `ask_question()`  
**Lines**: 61-111

```python
async def ask_question(query, query_language_code)
â”œâ”€â”€ Determines response language from config.SUPPORTED_LANGUAGES
â”œâ”€â”€ If non-English query:
â”‚   â””â”€â”€ Translates to English using SarvamService
â”œâ”€â”€ Executes RAG chain via RAGService
â”œâ”€â”€ If RAG fails:
â”‚   â””â”€â”€ Falls back to general LLM response
â””â”€â”€ Returns {"answer": str, "sources": List[str]}
```

#### 2. RAG Service Processing
**File**: `services/rag_service.py`  
**Class**: `RAGService`  
**Function**: `get_answer()`  
**Lines**: 45-55

```python
async def get_answer(question, response_language)
â”œâ”€â”€ Invokes RAG chain with question and response language
â”œâ”€â”€ Chain components:
â”‚   â”œâ”€â”€ Retriever: Gets relevant documents from vectorstore
â”‚   â”œâ”€â”€ Context formatter: Formats retrieved documents
â”‚   â”œâ”€â”€ Prompt: QA_PROMPT_TEMPLATE with context and question
â”‚   â”œâ”€â”€ LLM: ChatGroq with llama3-8b-8192
â”‚   â””â”€â”€ Parser: StrOutputParser for text output
â””â”€â”€ Returns generated answer string
```

#### 3. Document Retrieval
**File**: `services/rag_service.py`  
**Lines**: 22-26, 34-43
```python
# Retriever initialization
self.retriever = vectorstore.as_retriever(
    search_type=config.RETRIEVAL_SEARCH_TYPE,
    search_kwargs={"k": config.RETRIEVAL_K}
)

# RAG Chain execution
rag_chain = {
    "context": lambda x: format_docs(retriever.invoke(x["question"])),
    "question": lambda x: x["question"], 
    "response_language": lambda x: x["response_language"]
} | prompt | llm | parser
```

#### 4. Multilingual Translation (if needed)
**File**: `services/sarvam_service.py`  
**Class**: `SarvamService`  
**Function**: `translate_text()`
```python
async def translate_text(text, source_language_code, target_language_code)
â”œâ”€â”€ Makes API call to Sarvam AI translation endpoint
â”œâ”€â”€ Handles 11 supported Indian languages + English
â””â”€â”€ Returns translated text
```

#### 5. LLM Fallback Processing
**File**: `services/llm_service.py`  
**Class**: `LLMService`  
**Function**: `get_general_response()`  
**Lines**: 18-37

```python
async def get_general_response(query, target_language)
â”œâ”€â”€ Creates system prompt with target language instruction
â”œâ”€â”€ Uses AsyncOpenAI client with GPT model
â”œâ”€â”€ Applies temperature=0.7 for creativity
â””â”€â”€ Returns general knowledge response
```

### **Vector Store Management**
**File**: `services/document_service.py`  
**Class**: `DocumentProcessor`  
**Function**: `get_vectorstore()`  
**Lines**: 107-127

```python
def get_vectorstore(recreate=False, documents=None)
â”œâ”€â”€ If recreating: Creates new Chroma vectorstore
â”œâ”€â”€ If loading: Loads existing Chroma from CHROMA_DB_PATH
â”œâ”€â”€ Uses OpenAIEmbeddings for embedding function
â””â”€â”€ Returns Chroma vectorstore instance
```

---

## Complete E2E Request-Response Flow Summary

### **From Entry Point to Response: Complete Execution Path**

#### **Entry Point â†’ Initialization â†’ Processing â†’ Response**

```
ðŸš€ SYSTEM STARTUP
run_profai_websocket.py (main entry)
â”œâ”€â”€ Port checks (5001 FastAPI, 8765 WebSocket)
â”œâ”€â”€ FastAPI server start (background thread)
â”‚   â””â”€â”€ app.py initialization with all services
â””â”€â”€ WebSocket server start (main thread)
    â””â”€â”€ websocket_server.py connection handling

ðŸ“± CLIENT REQUEST FLOW
Client connects to ws://localhost:8765
â”œâ”€â”€ websocket_server.py: handle_client()
â”œâ”€â”€ Message parsing and routing
â”œâ”€â”€ Service processing (ChatService/AudioService/TeachingService)
â”œâ”€â”€ External API calls (OpenAI/Sarvam/Groq)
â”œâ”€â”€ Response generation
â””â”€â”€ WebSocket response to client

ðŸ“Š DATA PROCESSING PIPELINE
PDF Upload â†’ Text Extraction â†’ Chunking â†’ Vectorization â†’ Course Generation
Chat Query â†’ Translation â†’ RAG Retrieval â†’ LLM Processing â†’ Response
TTS Request â†’ Sarvam AI â†’ Audio Generation â†’ Streaming to Client
```

---

## E2E Flow #3: Text-to-Speech (TTS) Audio Generation

### **Trigger**: TTS request via WebSocket message

### **Entry Point**
**File**: `websocket_server.py` â†’ `services/audio_service.py`  
**Class**: `AudioService`

### **Flow Sequence**

#### 1. Audio Service Processing
**Function**: `generate_audio_from_text()`  
**Lines**: 22-37
```python
async def generate_audio_from_text(text, language, ultra_fast=False)
â”œâ”€â”€ Determines effective language (defaults to first supported)
â”œâ”€â”€ If ultra_fast=True:
â”‚   â””â”€â”€ Calls sarvam_service.generate_audio_ultra_fast()
â”œâ”€â”€ Else:
â”‚   â””â”€â”€ Calls sarvam_service.generate_audio()
â””â”€â”€ Returns io.BytesIO audio buffer
```

#### 2. Streaming Audio Generation
**Function**: `stream_audio_from_text()`  
**Lines**: 39-71
```python
async def stream_audio_from_text(text, language, websocket)
â”œâ”€â”€ Calls sarvam_service.stream_audio_generation()
â”œâ”€â”€ Yields audio chunks as they're generated
â”œâ”€â”€ Handles disconnection gracefully using connection_monitor utils
â”œâ”€â”€ Falls back to ultra_fast generation if streaming fails
â””â”€â”€ Async generator yielding audio chunk bytes
```

#### 3. Sarvam AI Integration
**File**: `services/sarvam_service.py`  
**Class**: `SarvamService`  
**Function**: `generate_audio()`
```python
async def generate_audio(text, language_code, speaker)
â”œâ”€â”€ Makes API call to Sarvam TTS endpoint
â”œâ”€â”€ Handles language-specific voice selection
â”œâ”€â”€ Returns audio data as BytesIO buffer
â””â”€â”€ Supports 11 Indian languages + English
```

#### 4. Connection Monitoring
**File**: `utils/connection_monitor.py`  
**Functions**: Multiple utilities for WebSocket state management
```python
is_client_connected(websocket) # Lines 81-113
â”œâ”€â”€ Checks websocket.state for OPEN status
â”œâ”€â”€ Validates connection health
â””â”€â”€ Returns boolean connection status

send_chunk_safely(websocket, chunk_data, client_id) # Lines 143-171  
â”œâ”€â”€ Validates connection before sending
â”œâ”€â”€ Sends JSON-encoded chunk data
â”œâ”€â”€ Handles ConnectionClosed exceptions
â””â”€â”€ Returns success boolean
```

---

## E2E Flow #4: Teaching Content Generation

### **Trigger**: Teaching content request via WebSocket

### **Entry Point**
**File**: `websocket_server.py` â†’ `services/teaching_service.py`  
**Class**: `TeachingService`

### **Complete Execution Path**
```
WebSocket Message (teaching request)
â†“
websocket_server.py: handle_client() - message parsing
â†“
services/teaching_service.py: generate_teaching_content()
â†“
services/llm_service.py: generate_response() - OpenAI API call
â†“
Content formatting for TTS
â†“
services/audio_service.py: generate_audio_from_text()
â†“
services/sarvam_service.py: generate_audio() - Sarvam AI API
â†“
WebSocket response with audio chunks
â†“
Client receives streamed audio response
```

### **Flow Sequence**

#### 1. Teaching Content Generation
**Function**: `generate_teaching_content()`  
**Lines**: 56-104
```python
async def generate_teaching_content(module_title, sub_topic_title, raw_content, language)
â”œâ”€â”€ Truncates content if >6000 chars to avoid timeout
â”œâ”€â”€ Creates comprehensive teaching prompt via _create_teaching_prompt()
â”œâ”€â”€ Generates content using LLMService with 5s timeout
â”œâ”€â”€ Formats content for TTS via _format_for_tts()
â”œâ”€â”€ Falls back to _create_fallback_content() if LLM fails
â””â”€â”€ Returns formatted teaching content string
```

#### 2. Streaming Teaching Content
**Function**: `generate_teaching_content_stream()`  
**Lines**: 16-54
```python
async def generate_teaching_content_stream(module_title, sub_topic_title, raw_content, language)
â”œâ”€â”€ Creates teaching prompt via _create_teaching_prompt()
â”œâ”€â”€ Streams content using llm_service.generate_response_stream()
â”œâ”€â”€ Yields chunks as they're generated
â”œâ”€â”€ Falls back to basic content if streaming fails
â””â”€â”€ Async generator yielding content chunks
```

#### 3. Teaching Prompt Creation
**Function**: `_create_teaching_prompt()`  
**Lines**: 106-153
```python
def _create_teaching_prompt(module_title, sub_topic_title, raw_content, language)
â”œâ”€â”€ Gets language-specific instruction via _get_language_instruction()
â”œâ”€â”€ Creates comprehensive teaching prompt with:
â”‚   â”œâ”€â”€ Context (module, topic, language)
â”‚   â”œâ”€â”€ Raw content to teach
â”‚   â”œâ”€â”€ Teaching instructions (9 detailed steps)
â”‚   â”œâ”€â”€ Teaching style guidelines
â”‚   â””â”€â”€ Response format requirements
â””â”€â”€ Returns complete prompt string
```

#### 4. Content Formatting for TTS
**Function**: `_format_for_tts()`  
**Lines**: 172-189
```python
def _format_for_tts(content)
â”œâ”€â”€ Adds natural pauses after punctuation (". " â†’ ". ... ")
â”œâ”€â”€ Adds longer pauses for paragraph breaks ("\n\n" â†’ " ... ... ")
â”œâ”€â”€ Ensures proper sentence endings
â”œâ”€â”€ Adds natural closing statement
â””â”€â”€ Returns TTS-optimized content string
```

#### 5. Multilingual Support
**Function**: `_get_language_instruction()`  
**Lines**: 155-170
```python
def _get_language_instruction(language)
â”œâ”€â”€ Maps language codes to native language instructions
â”œâ”€â”€ Supports 11 Indian languages + English
â”œâ”€â”€ Provides language-specific response instructions
â””â”€â”€ Returns localized instruction string
```

---

## Error Handling & Connection Management

### **WebSocket Connection Management**
**File**: `utils/connection_monitor.py`

#### Connection State Validation
```python
is_normal_closure(exception) # Lines 20-48
â”œâ”€â”€ Checks for ConnectionClosedOK exceptions
â”œâ”€â”€ Validates closure codes (1000=OK, 1001=Going Away)
â”œâ”€â”€ Analyzes error messages for normal closure indicators
â””â”€â”€ Returns boolean indicating normal vs error disconnect

validate_connection_before_operation(websocket, client_id, operation) # Lines 236-254
â”œâ”€â”€ Checks connection state before operations
â”œâ”€â”€ Logs disconnection with appropriate context
â””â”€â”€ Returns boolean operation permission
```

#### Connection Monitoring Class
```python
ConnectionStateMonitor # Lines 256-318
â”œâ”€â”€ Tracks connection metrics (chunks_sent, bytes_sent, disconnections)
â”œâ”€â”€ Monitors session duration and activity timestamps  
â”œâ”€â”€ Provides health checks with configurable idle timeouts
â””â”€â”€ Generates detailed connection diagnostics
```

### **Service Error Handling Patterns**

#### Chat Service
- Translation fallback to original text if Sarvam AI fails
- RAG to general LLM fallback if retrieval fails  
- Timeout handling for external API calls

#### Audio Service  
- Streaming to ultra_fast fallback if real-time fails
- Connection validation before each chunk send
- Graceful handling of client disconnections

#### Teaching Service
- Content truncation to prevent timeouts
- LLM generation to fallback content if AI fails
- Streaming to batch generation fallback

---

## Configuration & Dependencies

### **Key Configuration Files**
**File**: `config.py`
```python
# API Keys
OPENAI_API_KEY, GROQ_API_KEY, SARVAM_API_KEY

# Models  
LLM_MODEL_NAME = "gpt-4o-mini"
CURRICULUM_GENERATION_MODEL = "gpt-4o-mini"
CONTENT_GENERATION_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL_NAME = "text-embedding-3-small"

# Storage Paths
DOCUMENTS_DIR, VECTORSTORE_DIR, COURSES_DIR, OUTPUT_JSON_PATH
CHROMA_DB_PATH, CHROMA_COLLECTION_NAME

# Processing Parameters
CHUNK_SIZE = 1000, CHUNK_OVERLAP = 200, MAX_CHUNK_SIZE = 1500
RETRIEVAL_SEARCH_TYPE = "similarity", RETRIEVAL_K = 3

# Languages
SUPPORTED_LANGUAGES = [11 Indian languages + English with codes and names]
```

### **External Service Dependencies**
1. **OpenAI API**: GPT-4o-mini for content generation, embeddings
2. **Sarvam AI API**: TTS/STT/translation for Indian languages  
3. **Groq API**: Llama3-8b-8192 for RAG-based QA
4. **ChromaDB/FAISS**: Vector storage and similarity search
5. **FastAPI/WebSocket**: Real-time communication infrastructure

---

## Performance Considerations

### **Optimization Strategies**
1. **Content Truncation**: Teaching service limits content to 5500 chars
2. **Timeout Management**: 5s LLM timeouts, 8s OpenAI client timeouts
3. **Streaming**: Real-time audio and text generation with fallbacks
4. **Connection Validation**: Pre-operation WebSocket state checks
5. **Vector Store Persistence**: Reuse of embeddings across sessions
6. **Chunked Processing**: Document splitting for efficient retrieval

### **Scalability Features**
1. **Async/Await**: Full async processing pipeline
2. **Connection Monitoring**: Detailed metrics and health checks
3. **Graceful Degradation**: Multiple fallback layers
4. **Modular Services**: Independent, replaceable service components
5. **Language Support**: 12 total languages with unified interface

---

## Complete System Flow Summary

### **Primary Execution Paths in ProfAI_PROD**

1. **System Startup**: `run_profai_websocket.py` â†’ Dual server initialization (FastAPI + WebSocket)
2. **PDF Processing**: HTTP POST â†’ Document processing â†’ Course generation â†’ JSON response
3. **Chat Interaction**: WebSocket â†’ RAG processing â†’ LLM response â†’ Client delivery
4. **Audio Generation**: WebSocket â†’ TTS processing â†’ Audio streaming â†’ Client playback
5. **Teaching Content**: WebSocket â†’ Content generation â†’ Audio synthesis â†’ Educational delivery

### **Key Integration Points**
- **Entry Point**: `run_profai_websocket.py` (primary startup script)
- **Core Services**: All located in `services/` directory
- **External APIs**: OpenAI, Sarvam AI, Groq integrated via service layer
- **Data Storage**: Vector stores (ChromaDB/FAISS) for RAG operations
- **Response Delivery**: Real-time WebSocket streaming with fallback mechanisms

### **Architecture Verification**
âœ… **Confirmed**: This documentation covers **only ProfAI_PROD folder** components
âœ… **No references** to ProdAI-Roll folder or components
âœ… **Complete E2E flows** from `run_profai_websocket.py` entry point to response delivery
âœ… **All file paths** relative to ProfAI_PROD directory structure

---

## 5. Quiz Generation and Evaluation Flow

### **Module Quiz Generation (20 Questions)**

**HTTP Request**: `POST /api/quiz/generate-module`

**Entry Point**: `app.py` line 147 (`generate_module_quiz()`)

**Request Flow**:
1. **API Handler**: `app.py:147-186`
   - Validates quiz service availability
   - Loads course content from `config.OUTPUT_JSON_PATH`
   - Validates module week exists in course modules

2. **Quiz Service Initialization**: `services/quiz_service.py:24-33`
   - Creates quiz storage directories (`data/quizzes`, `data/quiz_answers`)
   - Initializes LLMService for question generation

3. **Module Quiz Generation**: `services/quiz_service.py:35-73`
   - `generate_module_quiz()` extracts specific module content
   - `_extract_module_content()` line 187 formats module text
   - `_create_module_quiz_prompt()` line 225 creates LLM prompt
   - LLMService generates 20 MCQ questions via OpenAI API

4. **Question Processing**: `services/quiz_service.py:274-305`
   - `_parse_quiz_response()` parses LLM output into structured questions
   - `_create_question_object()` creates QuizQuestion objects
   - Ensures exactly 20 questions with 4 options each

5. **Data Storage**: `services/quiz_service.py:307-337`
   - `_store_quiz()` saves quiz to JSON files
   - Stores answers separately for security in `quiz_answers` directory
   - Returns quiz without answers to client

**Response**: Quiz object with 20 questions (answers removed for security)

---

### **Course Quiz Generation (40 Questions)**

**HTTP Request**: `POST /api/quiz/generate-course`

**Entry Point**: `app.py` line 188 (`generate_course_quiz()`)

**Request Flow**:
1. **API Handler**: `app.py:188-217`
   - Loads complete course content from JSON file
   - Initiates comprehensive quiz generation

2. **Course Content Processing**: `services/quiz_service.py:75-114`
   - `generate_course_quiz()` extracts all module content
   - `_extract_all_course_content()` line 209 compiles full course text
   - Generates 40 questions in two 20-question batches for better quality

3. **Batch Question Generation**: `services/quiz_service.py:92-108`
   - `_create_course_quiz_prompt()` creates comprehensive prompts
   - Two separate LLM calls for questions 1-20 and 21-40
   - Combines results into single 40-question quiz

4. **Quiz Assembly and Storage**: Similar to module quiz
   - Validates 40 questions total
   - Stores quiz and answers separately
   - Returns display version without correct answers

**Response**: Quiz object with 40 questions covering entire course

---

### **Quiz Submission and Evaluation Flow**

**HTTP Request**: `POST /api/quiz/submit`

**Entry Point**: `app.py` line 219 (`submit_quiz()`)

**Request Flow**:
1. **API Handler**: `app.py:219-241`
   - Receives QuizSubmission with user answers
   - Validates quiz service availability

2. **Answer Evaluation**: `services/quiz_service.py:116-162`
   - `evaluate_quiz()` loads stored correct answers
   - `_load_quiz_answers()` retrieves answer key from secure storage
   - Compares user answers with correct answers

3. **Score Calculation**: `services/quiz_service.py:133-150`
   - Calculates total score and percentage
   - Determines pass/fail status (60% threshold)
   - Creates detailed results for each question

4. **Result Storage**: `services/quiz_service.py:341-362`
   - `_store_submission_result()` saves submission and results
   - Creates audit trail with timestamps
   - Returns QuizResult with score and detailed feedback

**Response**: QuizResult object with score, percentage, and detailed question-by-question results

---

### **Quiz Retrieval Flow**

**HTTP Request**: `GET /api/quiz/{quiz_id}`

**Entry Point**: `app.py` line 243 (`get_quiz()`)

**Request Flow**:
1. **API Handler**: `app.py:243-260`
   - Validates quiz_id parameter
   - Checks quiz service availability

2. **Secure Quiz Loading**: `services/quiz_service.py:164-185`
   - `get_quiz_without_answers()` loads quiz from storage
   - Removes correct answers and explanations for security
   - Returns display-safe quiz version

**Response**: Quiz object without correct answers for frontend display

---

### **Quiz API Integration Points**

**Data Storage Structure**:
```
ProfAI_PROD/data/
â”œâ”€â”€ quizzes/           # Quiz questions and metadata
â”‚   â””â”€â”€ {quiz_id}.json
â”œâ”€â”€ quiz_answers/      # Secure answer storage
â”‚   â”œâ”€â”€ {quiz_id}_answers.json
â”‚   â””â”€â”€ {quiz_id}_{user_id}_submission.json
```

**Security Features**:
- Correct answers stored separately from quiz questions
- Client never receives correct answers until submission
- Answer validation occurs server-side only
- Submission audit trail with timestamps

**External Dependencies**:
- **LLMService**: OpenAI GPT-4o-mini for question generation
- **Course Content**: Requires generated course JSON from DocumentService
- **File System**: JSON-based storage for quizzes and results

---

This documentation provides a complete mapping of all E2E flows in the **ProfAI_PROD system only**, including the new quiz generation and evaluation capabilities, enabling developers to understand the full request lifecycle from the `run_profai_websocket.py` entry point through service processing to response delivery.
